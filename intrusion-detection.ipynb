{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following tasks:\n",
    "\n",
    "- For each frame of an input video, the system needs to detect and display objects not belonging to the background scene and produce a textual output listing the found blobs and their meaningful features.\n",
    "- The system is required to discriminate between a present blob and a false one originated by the removal of an object from the background reference.\n",
    "\n",
    "The proposed solution implements the developed library `intrusiondetection`, this notebook shows the step-by-step operations computed to achieve the final outputs displaying the computations on some key frames alongside considerations over the made choices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from intrusiondetection.parameters import ParameterList, ParameterSet\n",
    "from intrusiondetection.utility import distance_euclidean, subplot_images, subplot_images_frames, subplot_morphology_steps\n",
    "from intrusiondetection.morphology import MorphOp, MorphOpsSet\n",
    "from intrusiondetection.video import Video \n",
    "from intrusiondetection.displayable import Background\n",
    "from intrusiondetection.enum import BackgroundMethod\n",
    "from intrusiondetection.presets import default_preset\n",
    "from intrusiondetection.enum import ParameterPreset\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "input_video_path = \"input.avi\"\n",
    "video = Video(input_video_path)\n",
    "\n",
    "key_frame_indexes = [124, 164, 301]\n",
    "key_frames = [video.frames[frame_index] for frame_index in key_frame_indexes]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations\n",
    "\n",
    "Video characteristics:\n",
    "\n",
    "- 12 fps\n",
    "- ~41s\n",
    "- 320x240 pixels\n",
    "- 8 bit/pixel (256 gray levels)\n",
    "\n",
    "Observed details:\n",
    "\n",
    "- There is a meaningful amount of noise present.\n",
    "- The lighting condition in the scene changes slightly.\n",
    "- The person changes its moving velocity in the course of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Background\n",
    "\n",
    "To estimate a good background for the scene, it's necessary to perform an interpolation between some chosen frames of the video, the parameters of the operation are:\n",
    "\n",
    "- The interpolation function used, the methods took in consideration are `np.mean` and `np.median`.\n",
    "- The amount of initial frames, which is tuned based on a tradeoff between the smallest and most stable number. Therefore, the considered values are `40`, `80`, `120`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing various tests, it has been noticed that the output doesn't result in a significant change of quality by increasing the value over 80 frames, on the other hand, by decreasing the value under 80, the resulting image looks unclear using both functions.\n",
    "\n",
    "It has also been observed that the `np.median` function achieves more stable solutions.\n",
    "\n",
    "The chosen values are `80` frames using the `np.median` interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_background = Background(input_video_path, np.median, 80)\n",
    "initial_background.display('image', title='Median of 80 pixels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background update\n",
    "The two main approaches to obtain a dynamic background are `blind` and `selective`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section shows an example of a blind background computed using an adaption rate of `0.3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_alpha = 0.3\n",
    "\n",
    "blind_backgrounds = video.process_backgrounds(BackgroundMethod.BLIND, initial_background, background_alpha)\n",
    "key_frame_backgrounds = [blind_backgrounds[key_frame_index] for key_frame_index in key_frame_indexes]\n",
    "\n",
    "subplot_images_frames(key_frame_backgrounds, key_frame_indexes, 'image', \"Frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the selective approach, a threshold and a distance function have to be used to compute the subtraction between the frame and the background.\n",
    "\n",
    "As a further improvement, some binary morphology operators are applied to obtain a more meaningful background mask:\n",
    "\n",
    "- 3x3 Opening: Denoising of the subtraction\n",
    "- 50x50 Closing: Filling of any potential holes\n",
    "- 15x15 Dilation: Achievement of a mask big enough to contain the detected objects and their position after any movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_threshold = 30\n",
    "background_distance = distance_euclidean\n",
    "\n",
    "selective_backgrounds = video.process_backgrounds(\n",
    "    BackgroundMethod.SELECTIVE, initial_background, background_alpha, background_threshold, background_distance, \n",
    "    MorphOpsSet(\n",
    "        MorphOp(cv2.MORPH_OPEN, (3,3)), \n",
    "        MorphOp(cv2.MORPH_CLOSE, (50,50), cv2.MORPH_ELLIPSE), \n",
    "        MorphOp(cv2.MORPH_DILATE, (15,15), cv2.MORPH_ELLIPSE)\n",
    "    )\n",
    ")\n",
    "key_frame_backgrounds = [selective_backgrounds[key_frame_index] for key_frame_index in key_frame_indexes]\n",
    "\n",
    "for key_frame_background in key_frame_backgrounds:\n",
    "    key_frame_background.display_row([\n",
    "        {'key': 'subtraction', 'title': 'Subtraction'},\n",
    "        {'key': 'mask_raw', 'title': 'Mask'},\n",
    "        {'key': 'mask_refined', 'title': 'Mask after binary morphology'},\n",
    "        {'key': 'image', 'title': 'Final Result'},\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A selective background clearly obtains better results, it is therefore the chosen method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Subtraction\n",
    "\n",
    "To perform the actual change detection the first step is to subtract the current frame with respect to the corresponding background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 15\n",
    "distance_function = distance_euclidean\n",
    "\n",
    "for key_frame_index, key_frame in zip(key_frame_indexes, key_frames):\n",
    "    key_frame.apply_change_detection(selective_backgrounds[key_frame_index], threshold, distance_function)\n",
    "\n",
    "subplot_images_frames(key_frames, key_frame_indexes, ['image', 'mask_raw'], \"Frame\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the selective background update, the resulting mask doesn't show a big amount of noise, a series of binary morphology operators is applied to improve furthermore the result:\n",
    "\n",
    "- 3x3 Rectangular Opening: Removal of the majority of the noise\n",
    "- 50x50 Closing: Execution of holes filling\n",
    "- 10x10 Opening: Deletion of small blobs due to the remaining noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_ops = [\n",
    "    MorphOp(cv2.MORPH_OPEN, (3,3)),\n",
    "    MorphOp(cv2.MORPH_CLOSE, (50, 50), cv2.MORPH_ELLIPSE),\n",
    "    MorphOp(cv2.MORPH_OPEN, (10,10), cv2.MORPH_ELLIPSE),\n",
    "]\n",
    "\n",
    "for key_frame in key_frames:\n",
    "    subplot_morphology_steps(key_frame, morph_ops)\n",
    "    key_frame.apply_morphology_operators(MorphOpsSet(*morph_ops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blob Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blob Labeling\n",
    "\n",
    "The labeling of the obtained image is performed using the `cv2.connectedComponents` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_frame in key_frames:\n",
    "    key_frame.apply_blob_labeling(create_output=True)\n",
    "\n",
    "subplot_images_frames(key_frames, key_frame_indexes, 'blobs_labeled', 'Frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected Features\n",
    "\n",
    "The considered features are:\n",
    "\n",
    "- Area\n",
    "- Perimeter\n",
    "- Barycentre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Remapping\n",
    "To keep the IDs assignment consistent during the whole process and assure uniqueness it is necessary to keep continuity between the blob labeling in subsequential frames, therefore, a similarity function is computed to seek for correspondances between the current and previous blobs.\n",
    "\n",
    "The IDs are then assigned by matching the ones in the previous frame.\n",
    "The blobs are displayed with their ID printed on the barycentre.\n",
    "\n",
    "Given the area of the two objects $A_1, A_2$, their barycentres $(x_1, y_1), (x_2, y_2)$, the total area of the frame $A_F$ and its diagonal $d_F$ the similarity function is defined as:\n",
    "\n",
    "\n",
    "$$\\frac{\\frac{A_1-A_2}{A_F}+\\frac{\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}}{d_F}}{2}$$\n",
    "\n",
    "A threshold parameter is then applied to define a lower bound for a minimum similarity below which two blobs are always considered different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 80\n",
    "\n",
    "video_copy = Video(input_video_path)\n",
    "#Computes the whole intrusion detection algorithm at once to get the real assigned IDs\n",
    "video_copy.intrusion_detection(ParameterSet(tuning_params=default_preset(ParameterPreset.SLOW)), initial_background)\n",
    "key_frame_prevs = []\n",
    "\n",
    "for key_frame, key_frame_index in zip(key_frames, key_frame_indexes):\n",
    "    key_frame_prev = video_copy.frames[key_frame_index - 1]\n",
    "    \n",
    "    key_frame.apply_blob_remapping(key_frame_prev.blobs, similarity_threshold, base_id=key_frame_prev.max_blob_id, create_output=True)\n",
    "    key_frame_prevs.append(key_frame_prev)\n",
    "\n",
    "subplot_images_frames(key_frame_prevs, key_frame_indexes, 'blobs_remapped', \"Frame\")\n",
    "subplot_images_frames(key_frames, key_frame_indexes, 'blobs_remapped', \"Frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification \n",
    "To classify the blobs as `person` or `object`, a `classification_score` is computed. The value is based on their area by normalizing it with respect to the total number of pixels in the frame.\n",
    "\n",
    "After some analysis of the video, the chosen threshold for the classification is `2.6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_threshold = 2.6\n",
    "\n",
    "for key_frame in key_frames:\n",
    "    key_frame.apply_classification(classification_threshold, create_output=True)\n",
    "\n",
    "subplot_images_frames(key_frames, key_frame_indexes, 'blobs_classified', 'Frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True/False Object Recognition\n",
    "\n",
    "To detect whether an object is effectively present or not, the selected approach is the evaluation of the edge strength, such that, when the contours of a found blob presents sharp edges in the original image, it is considered to be present, otherwise it is labeled as fake.\n",
    "\n",
    "The computation is performed by using a Sobel operator returning a smoothed edge score, not taking into account the large amount of noise present in the original frame. \n",
    "\n",
    "Given the fact that in the majority of the situations, the presence/absence of the object is continuous between contiguous frames, it has been chosen to use a value obtained by computing a weighted sum between the current edge score and the score of the correspondent object in the precedent frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_threshold = 92\n",
    "edge_adaptation = 0.1\n",
    "\n",
    "for key_frame in key_frames:\n",
    "    key_frame.apply_object_recognition(edge_threshold, edge_adaptation, create_output=True)\n",
    "\n",
    "subplot_images_frames(key_frames, key_frame_indexes, 'blobs_detected', 'Frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Text Output Generation\n",
    "\n",
    "A CSV file is then generated, for each frame the following informations are stored:\n",
    "\n",
    "- Frame Index\n",
    "- Number of Detected Objects\n",
    "- A row for each detected object containing:\n",
    "   - Object Identifier\n",
    "   - Area\n",
    "   - Perimeter\n",
    "   - Barycentre (x coordinate)\n",
    "   - Barycentre (y coordinate)\n",
    "   - Classification Score\n",
    "   - Edge Score\n",
    "   - Object Presence [True / False] \n",
    "   - Classification\n",
    "\n",
    "## Video Output Generation\n",
    "\n",
    "The graphical output shows the contours of the found objects, the color of the contour depends on the object classification:\n",
    "\n",
    "- Person: Blue\n",
    "- True Object: Green\n",
    "- Fake Object: Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_frame in key_frames:\n",
    "    key_frame.generate_graphical_output()\n",
    "\n",
    "subplot_images_frames(key_frames, key_frame_indexes, 'image_output', 'Frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Time\n",
    "\n",
    "To improve the execution time a series of different presets have been provided to allow adjustment of the tradeoff between precision and execution time.\n",
    "\n",
    "The only main difference consists in the usage of rectangular structuring elements instead of elliptical ones in the binary morphology step. While reducing the correctness of the final shapes the time of the execution drops significantly.\n",
    "\n",
    "| Preset | Max Time (per frame) | Min Time (per frame) | Avg Time (per frame) |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 (SLOW) | 134ms | 44ms | 60ms |\n",
    "| 2 (MEDIUM) | 35ms | 24ms | 30ms |\n",
    "| 3 (FAST) | 17ms | 5ms | 9ms |\n",
    "\n",
    "In this notebook, the preset `1` was used, but for example, by using the preset `3`, an average time of 9ms allows the algorithm to be performed online with a relatively high frame rate.\n",
    "\n",
    "## Usage\n",
    "\n",
    "`python main.py [-I/--input INPUT] [-o/--output OUTPUT_DIRECTORY] [-S/--stats] [-T/--tuning] [-P/--preset PRESET]`\n",
    "\n",
    "- `--input`: Input video used to compute the intrusion detection algorithm\n",
    "- `--ouput`: Output directory where the requested output are stored\n",
    "- `--stats`: Compute and print additional info on the elaborated data\n",
    "- `--tuning`: Activates tuning mode, in which all the algorithm steps are generated as output videos\n",
    "- `--preset`: Preset of parameters used from most accurate (1) to fastest (3)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Marco Cucè"
   },
   {
    "name": "Alessandro Stockman"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_metadata": {
   "date": ""
  },
  "subtitle": "Test",
  "title": "Intrusion Detection System - Project Work"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
