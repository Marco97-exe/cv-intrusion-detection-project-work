{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrusion Detection Computer Vision System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following tasks:\n",
    "- For each frame of an input video, the system needs to detect and display object not belonging to the background scene and produce a textual output listing the found blobs and their meaningful features.\n",
    "- The system is required to discriminate between a present blob and a false one originated by the removal of an object from the background reference.\n",
    "\n",
    "The proposed solution implements the developed library `intrusiondetection`, this notebook shows the step-by-step operations computed to achieve the final outputs displaying the computations on some key frames alongside considerations over the made choices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from intrusiondetection.parameters import ParameterList\n",
    "from intrusiondetection.utility import distance_euclidean\n",
    "from intrusiondetection.model import MorphOp, MorphOpsSet, Video, Background\n",
    "\n",
    "# Only for jupyter notebook visualization\n",
    "%matplotlib inline \n",
    "\n",
    "input_video_path = \"rilevamento-intrusioni-video.avi\"\n",
    "video = Video(input_video_path)\n",
    "\n",
    "key_frame_index_1 = 124\n",
    "key_frame_index_2 = 150\n",
    "key_frame_index_3 = 317"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Considerations\n",
    "\n",
    "Video characteristics:\n",
    "- 12 fps\n",
    "- ~41s\n",
    "- 320x240 pixels\n",
    "- 8 bit/pixel (256 gray levels)\n",
    "\n",
    "Observed details:\n",
    "- There is a meaningful amount of noise present.\n",
    "- The lighting condition in the scene changes slightly.\n",
    "- The person changes its moving velocity in the course of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Background Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Initial Background\n",
    "\n",
    "To estimate a good background for the scene, it's necessary to perform an interpolation between some chosen frames of the video, the parameters of the operation are:\n",
    "\n",
    "- The interpolation function used, the methods took in consideration are `np.mean` and `np.median`.\n",
    "- The amount of initial frames, which is tuned based on a tradeoff between the smallest and most stable number. Therefore, the considered values are `40`, `80`, `120`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intrusiondetection.utility import subplot_images\n",
    "\n",
    "bg_medn_40 = Background(input_video_path, np.median, 40)\n",
    "bg_medn_80 = Background(input_video_path, np.median, 80)\n",
    "bg_medn_120 = Background(input_video_path, np.median, 120)\n",
    "bg_mean_40 = Background(input_video_path, np.mean, 40)\n",
    "bg_mean_80 = Background(input_video_path, np.mean, 80)\n",
    "bg_mean_120 = Background(input_video_path, np.mean, 120)\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': bg_medn_40,\n",
    "        'key': 'image',\n",
    "        'title': 'Median of 40 pixels'\n",
    "    },\n",
    "    {\n",
    "        'object': bg_medn_80,\n",
    "        'key': 'image',\n",
    "        'title': 'Median of 80 pixels'\n",
    "    },\n",
    "    {\n",
    "        'object': bg_medn_120,\n",
    "        'key': 'image',\n",
    "        'title': 'Median of 120 pixels'\n",
    "    }\n",
    "])\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': bg_mean_40,\n",
    "        'key': 'image',\n",
    "        'title': 'Mean of 40 pixels'\n",
    "    },\n",
    "    {\n",
    "        'object': bg_mean_80,\n",
    "        'key': 'image',\n",
    "        'title': 'Mean of 80 pixels'\n",
    "    },\n",
    "    {\n",
    "        'object': bg_mean_120,\n",
    "        'key': 'image',\n",
    "        'title': 'Mean of 120 pixels'\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing various tests, it has been noticed that the output doesn't result in a significant change of quality by increasing the value over 80 frames, simultaneously, by decreasing the value under 80, the resulting image looks unclear using both functions.\n",
    "\n",
    "It has also been observed that the `np.median` function achieves more stable solutions.\n",
    "\n",
    "The chosen values are `80` frames using the `np.median` interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bg_medn_80' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0b31a054045d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minitial_background\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbg_medn_80\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bg_medn_80' is not defined"
     ]
    }
   ],
   "source": [
    "initial_background = bg_medn_80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Background update\n",
    "The two main approaches to obtain a dynamic background are `blind` and `selective`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section shows an example of a blind background computed using an adaption rate of `0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blind_backgrounds = video.process_backgrounds('blind', initial_background, 0.1)\n",
    "key_frame_background_1 = blind_backgrounds[key_frame_index_1]\n",
    "key_frame_background_2 = blind_backgrounds[key_frame_index_2]\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': key_frame_background_1,\n",
    "        'key': 'image',\n",
    "        'title': ''\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_background_2,\n",
    "        'key': 'image',\n",
    "        'title': ''\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the selective approach, a threshold and a distance function have to be used to compute the subtraction between the frame and the background.\n",
    "\n",
    "As a further improvement, some binary morphology operators are applied to obtain a more meaningful background mask:\n",
    "- 3x3 Opening: Denoising of the subtraction\n",
    "- 3x3 Closing: Filling of any potential holes\n",
    "- 25x25 Dilation: Achievement of a mask big enough to contain the detected objects and their position after any movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selective_backgrounds = video.process_backgrounds('selective', initial_background, 0.1, 37, distance_euclidean, MorphOpsSet(\n",
    "    MorphOp(cv2.MORPH_OPEN, (3,3), iterations=1),\n",
    "    MorphOp(cv2.MORPH_CLOSE, (3,3), iterations=1),\n",
    "    MorphOp(cv2.MORPH_DILATE, (25,25), cv2.MORPH_ELLIPSE)\n",
    "))\n",
    "key_frame_background_1 = selective_backgrounds[key_frame_index_1]\n",
    "key_frame_background_2 = selective_backgrounds[key_frame_index_2]\n",
    "\n",
    "key_frame_background_1.display_row([\n",
    "    {\n",
    "        'key': 'subtraction',\n",
    "        'title': 'Subtraction'\n",
    "    },\n",
    "    {\n",
    "        'key': 'mask_raw',\n",
    "        'title': 'Mask'\n",
    "    },\n",
    "    {\n",
    "        'key': 'mask_refined',\n",
    "        'title': 'Mask after binary morphology'\n",
    "    },\n",
    "    {\n",
    "        'key': 'image',\n",
    "        'title': 'Final Result'\n",
    "    },\n",
    "])\n",
    "\n",
    "key_frame_background_2.display_row([\n",
    "    {\n",
    "        'key': 'subtraction',\n",
    "        'title': 'Subtraction'\n",
    "    },\n",
    "    {\n",
    "        'key': 'mask_raw',\n",
    "        'title': 'Mask'\n",
    "    },\n",
    "    {\n",
    "        'key': 'mask_refined',\n",
    "        'title': 'Mask after binary morphology'\n",
    "    },\n",
    "    {\n",
    "        'key': 'image',\n",
    "        'title': 'Final Result'\n",
    "    },\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A selective background clearly obtains better results, it is therefore the chosen method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Change Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Background Subtraction\n",
    "\n",
    "To perform the actual change detection the first step is to subtract the current frame with respect to the corresponding background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_frame_1 = video.frames[key_frame_index_1]\n",
    "key_frame_1.apply_change_detection(selective_backgrounds[key_frame_index_1], 37, distance_euclidean)\n",
    "\n",
    "key_frame_2 = video.frames[key_frame_index_2]\n",
    "key_frame_2.apply_change_detection(selective_backgrounds[key_frame_index_2], 37, distance_euclidean)\n",
    "\n",
    "key_frame_3 = video.frames[key_frame_index_3]\n",
    "key_frame_3.apply_change_detection(selective_backgrounds[key_frame_index_3], 37, distance_euclidean)\n",
    "\n",
    "key_frame_1.display_row([\n",
    "    {\n",
    "        'key': 'subtraction',\n",
    "        'title': 'Subtraction'\n",
    "    },\n",
    "    {\n",
    "        'key': 'mask_raw',\n",
    "        'title': 'Thresholded Mask'\n",
    "    }\n",
    "])\n",
    "\n",
    "key_frame_2.display_row([\n",
    "    {\n",
    "        'key': 'subtraction',\n",
    "        'title': 'Subtraction'\n",
    "    },\n",
    "    {\n",
    "        'key': 'mask_raw',\n",
    "        'title': 'Thresholded Mask'\n",
    "    }\n",
    "])\n",
    "\n",
    "key_frame_3.display_row([\n",
    "    {\n",
    "        'key': 'subtraction',\n",
    "        'title': 'Subtraction'\n",
    "    },\n",
    "    {\n",
    "        'key': 'mask_raw',\n",
    "        'title': 'Thresholded Mask'\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Binary Morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting mask shows a big amount of noise and holes inside the objects, a series of binary morphology operators is applied:\n",
    "- 3x3 Rectangular Opening: Removal of the majority of the noise\n",
    "- 45x70 Closing: Execution of holes filling\n",
    "- 6x6 Opening: Deletion of small blobs due to the remaining noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1 = MorphOpsSet(\n",
    "    MorphOp(cv2.MORPH_OPEN, (3,3))\n",
    ")\n",
    "\n",
    "ms2 = MorphOpsSet(\n",
    "    MorphOp(cv2.MORPH_OPEN, (3,3)),\n",
    "    MorphOp(cv2.MORPH_CLOSE, (45,70), cv2.MORPH_ELLIPSE),\n",
    ")\n",
    "\n",
    "ms3 = MorphOpsSet(\n",
    "    MorphOp(cv2.MORPH_OPEN, (3,3)),\n",
    "    MorphOp(cv2.MORPH_CLOSE, (45,70), cv2.MORPH_ELLIPSE),\n",
    "    MorphOp(cv2.MORPH_OPEN, (6,6), cv2.MORPH_ELLIPSE),\n",
    ")\n",
    "\n",
    "frame1_op1, frame1_op2 = key_frame_1.copy(), key_frame_1.copy()\n",
    "frame1_op1.apply_morphology_operators(ms1)\n",
    "frame1_op2.apply_morphology_operators(ms2)\n",
    "key_frame_1.apply_morphology_operators(ms3)\n",
    "\n",
    "frame2_op1, frame2_op2 = key_frame_2.copy(), key_frame_2.copy()\n",
    "frame2_op1.apply_morphology_operators(ms1)\n",
    "frame2_op2.apply_morphology_operators(ms2)\n",
    "key_frame_2.apply_morphology_operators(ms3)\n",
    "\n",
    "frame3_op1, frame3_op2 = key_frame_3.copy(), key_frame_3.copy()\n",
    "frame3_op1.apply_morphology_operators(ms1)\n",
    "frame3_op2.apply_morphology_operators(ms2)\n",
    "key_frame_3.apply_morphology_operators(ms3)\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': frame1_op1,\n",
    "        'key': 'mask_refined',\n",
    "        'title': 'Operation'\n",
    "    },\n",
    "    {\n",
    "        'object': frame1_op2,\n",
    "        'key': 'mask_refined',\n",
    "        'title': 'Operation'\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_1,\n",
    "        'key': 'mask_refined',\n",
    "        'title': 'Operation'\n",
    "    }\n",
    "])\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': frame2_op1,\n",
    "        'key': 'mask_refined',\n",
    "        'title': 'Operation'\n",
    "    },\n",
    "    {\n",
    "        'object': frame2_op2,\n",
    "        'key': 'mask_refined',\n",
    "        'title': 'Operation'\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_2,\n",
    "        'key': 'mask_refined',\n",
    "        'title': 'Operation'\n",
    "    }\n",
    "])\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': frame3_op1,\n",
    "        'key': 'mask_refined',\n",
    "        'title': 'Operation'\n",
    "    },\n",
    "    {\n",
    "        'object': frame3_op2,\n",
    "        'key': 'mask_refined',\n",
    "        'title': 'Operation'\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_3,\n",
    "        'key': 'mask_refined',\n",
    "        'title': 'Operation'\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Blob Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Blob Labeling\n",
    "\n",
    "The labeling of the obtained image is performed using the `cv2.connectedComponents` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_frame_1.apply_blob_labeling()\n",
    "key_frame_2.apply_blob_labeling()\n",
    "key_frame_3.apply_blob_labeling()\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': key_frame_1,\n",
    "        'key': 'blobs_labeled',\n",
    "        'title': \"\",\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_2,\n",
    "        'key': 'blobs_labeled',\n",
    "        'title': \"\",\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_3,\n",
    "        'key': 'blobs_labeled',\n",
    "        'title': \"\",\n",
    "    },\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Selected Features\n",
    "\n",
    "The meaningful features considered are:\n",
    "- Area\n",
    "- Perimeter\n",
    "- Barycentre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Label Remapping\n",
    "It is necessary to keep continuity between the blob labeling in subsequential frames, therefore, a dissimilarity function is computed to seek for correspondances between the current and previous blobs.\n",
    "\n",
    "The label are then remapped to match the ones in the previous frame.\n",
    "The blobs are displayed with their label printed on the barycentre.\n",
    "\n",
    "The dissimilarity function is defined as: TODO\n",
    "\n",
    "The above function expects a threshold parameter to define an upper bound for a maximum dissimilarity above which two blobs are always considered different.\n",
    "\n",
    "TODO: Mostrare dissimilarity tra tutti gli oggetti per dare un'idea della media?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intrusiondetection.parameters import ParameterSet\n",
    "\n",
    "params = ParameterSet({\n",
    "    \"input_video\": \"rilevamento-intrusioni-video.avi\",\n",
    "    \"output_directory\": \"output\"\n",
    "}, {\n",
    "    \"threshold\": 37,\n",
    "    \"distance\": distance_euclidean,\n",
    "    \"morph_ops\": MorphOpsSet(\n",
    "        MorphOp(cv2.MORPH_OPEN, (3,3)),\n",
    "        MorphOp(cv2.MORPH_CLOSE, (45,70), cv2.MORPH_ELLIPSE),\n",
    "        MorphOp(cv2.MORPH_OPEN, (6,6), cv2.MORPH_ELLIPSE),\n",
    "    ),\n",
    "    \"background_threshold\": 37,\n",
    "    \"background_distance\": distance_euclidean,\n",
    "    \"background_morph_ops\": MorphOpsSet(MorphOp(cv2.MORPH_CLOSE, (3,3), iterations=1),MorphOp(cv2.MORPH_OPEN, (3,3), iterations=2), MorphOp(cv2.MORPH_DILATE, (25,10)), MorphOp(cv2.MORPH_ERODE, (15,5))),\n",
    "    \"alpha\": 0.1,\n",
    "    \"background\": {\n",
    "        \"frames\": [100],\n",
    "        \"interpolation\": [np.median]\n",
    "    },\n",
    "    \"similarity_threshold\": 5000,\n",
    "    \"classification_threshold\": 200,\n",
    "    \"edge_threshold\": 1000,\n",
    "})\n",
    "\n",
    "key_frame_prev_1 = video.frames[key_frame_index_1 - 1]\n",
    "key_frame_prev_1.intrusion_detection(params, selective_backgrounds[key_frame_index_1 - 1], [])\n",
    "\n",
    "key_frame_prev_2 = video.frames[key_frame_index_2 - 1]\n",
    "key_frame_prev_2.intrusion_detection(params, selective_backgrounds[key_frame_index_2 - 1], [])\n",
    "\n",
    "key_frame_prev_3 = video.frames[key_frame_index_3 - 1]\n",
    "key_frame_prev_3.intrusion_detection(params, selective_backgrounds[key_frame_index_3 - 1], [])\n",
    "\n",
    "#key_frame_1.apply_blob_analysis(key_frame_prev_1.blobs, 5000)\n",
    "#key_frame_2.apply_blob_analysis(key_frame_prev_2.blobs, 5000)\n",
    "key_frame_1.apply_blob_remapping(key_frame_prev_1.blobs, 5000)\n",
    "key_frame_2.apply_blob_remapping(key_frame_prev_2.blobs, 5000)\n",
    "key_frame_3.apply_blob_remapping(key_frame_prev_3.blobs, 5000)\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': key_frame_prev_1,\n",
    "        'key': 'blobs_remapped',\n",
    "        'title': 'Previous Frame Labeling'\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_1,\n",
    "        'key': 'blobs_remapped',\n",
    "        'title': 'Current Frame Labeling'\n",
    "    }\n",
    "])\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': key_frame_prev_2,\n",
    "        'key': 'blobs_remapped',\n",
    "        'title': 'Previous Frame Labeling'\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_2,\n",
    "        'key': 'blobs_remapped',\n",
    "        'title': 'Current Frame Labeling'\n",
    "    }\n",
    "])\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': key_frame_prev_3,\n",
    "        'key': 'blobs_remapped',\n",
    "        'title': 'Previous Frame Labeling'\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_3,\n",
    "        'key': 'blobs_remapped',\n",
    "        'title': 'Current Frame Labeling'\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Classification \n",
    "To classify the blobs as `person` or `object`, a `classification_score` is computed. The value is based on their features.\n",
    "\n",
    "The function is defined as: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_frame_1.apply_classification(2000)\n",
    "key_frame_2.apply_classification(2000)\n",
    "key_frame_3.apply_classification(2000)\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': key_frame_1,\n",
    "        'key': 'blobs_classified',\n",
    "        'title': ''\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_2,\n",
    "        'key': 'blobs_classified',\n",
    "        'title': ''\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_3,\n",
    "        'key': 'blobs_classified',\n",
    "        'title': ''\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. True/False Object Recognition\n",
    "\n",
    "To detect whether an object is effectively present or not, the selected approach is the evaluation of the edge strength, such that, when the contours of a found blob presents sharp edges in the original image, it is considered to be present, otherwise it is labeled as fake.\n",
    "\n",
    "The computation is performed by using a Sobel operator returning a smoothed edge score, not taking into account the large amount of noise present in the original frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_frame_1.apply_object_recognition(2000)\n",
    "key_frame_2.apply_object_recognition(2000)\n",
    "key_frame_3.apply_object_recognition(2000)\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': key_frame_1,\n",
    "        'key': 'blobs_detected',\n",
    "        'title': ''\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_2,\n",
    "        'key': 'blobs_detected',\n",
    "        'title': ''\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_3,\n",
    "        'key': 'blobs_detected',\n",
    "        'title': ''\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Output Generation\n",
    "\n",
    "### 6.1. Text Output\n",
    "\n",
    "A CSV file is then generated, for each frame the following informations are stored:\n",
    "- Frame Index\n",
    "- Number of Detected Objects\n",
    "- Then a row for each detected object is printed listing this features:\n",
    "   - Object Label\n",
    "   - Area\n",
    "   - Perimeter\n",
    "   - Classification\n",
    "\n",
    "### 6.2. Video Output\n",
    "\n",
    "The graphical output shows the contours of the found objects, the color of the contour depends on the object classification:\n",
    "- Person: Blue\n",
    "- True Object: Green\n",
    "- Fake Object: Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_frame_1.generate_output()\n",
    "key_frame_2.generate_output()\n",
    "key_frame_3.generate_output()\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': key_frame_1,\n",
    "        'key': 'image_output',\n",
    "        'title': ''\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_2,\n",
    "        'key': 'image_output',\n",
    "        'title': ''\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_3,\n",
    "        'key': 'image_output',\n",
    "        'title': ''\n",
    "    }\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
