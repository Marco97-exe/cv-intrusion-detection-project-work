{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrusion Detection Computer Vision System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following tasks:\n",
    "- For each frame of an input video, the system needs to detect and display objects not belonging to the background scene and produce a textual output listing the found blobs and their meaningful features.\n",
    "- The system is required to discriminate between a present blob and a false one originated by the removal of an object from the background reference.\n",
    "\n",
    "The proposed solution implements the developed library `intrusiondetection`, this notebook shows the step-by-step operations computed to achieve the final outputs displaying the computations on some key frames alongside considerations over the made choices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from intrusiondetection.parameters import ParameterList\n",
    "from intrusiondetection.utility import distance_euclidean\n",
    "from intrusiondetection.morphology import MorphOp, MorphOpsSet\n",
    "from intrusiondetection.video import Video \n",
    "from intrusiondetection.displayable import Background\n",
    "from intrusiondetection.enum import BackgroundMethod\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "input_video_path = \"rilevamento-intrusioni-video.avi\"\n",
    "video = Video(input_video_path)\n",
    "\n",
    "key_frame_indexes = [124, 150, 325]\n",
    "key_frames = [video.frames[frame_index] for frame_index in key_frame_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Considerations\n",
    "\n",
    "Video characteristics:\n",
    "- 12 fps\n",
    "- ~41s\n",
    "- 320x240 pixels\n",
    "- 8 bit/pixel (256 gray levels)\n",
    "\n",
    "Observed details:\n",
    "- There is a meaningful amount of noise present.\n",
    "- The lighting condition in the scene changes slightly.\n",
    "- The person changes its moving velocity in the course of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Background Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Initial Background\n",
    "\n",
    "To estimate a good background for the scene, it's necessary to perform an interpolation between some chosen frames of the video, the parameters of the operation are:\n",
    "\n",
    "- The interpolation function used, the methods took in consideration are `np.mean` and `np.median`.\n",
    "- The amount of initial frames, which is tuned based on a tradeoff between the smallest and most stable number. Therefore, the considered values are `40`, `80`, `120`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intrusiondetection.utility import subplot_images\n",
    "\n",
    "bg_medn_40 = Background(input_video_path, np.median, 40)\n",
    "bg_medn_80 = Background(input_video_path, np.median, 80)\n",
    "bg_medn_120 = Background(input_video_path, np.median, 120)\n",
    "bg_mean_40 = Background(input_video_path, np.mean, 40)\n",
    "bg_mean_80 = Background(input_video_path, np.mean, 80)\n",
    "bg_mean_120 = Background(input_video_path, np.mean, 120)\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': bg_medn_40,\n",
    "        'key': 'image',\n",
    "        'title': 'Median of 40 pixels'\n",
    "    },\n",
    "    {\n",
    "        'object': bg_medn_80,\n",
    "        'key': 'image',\n",
    "        'title': 'Median of 80 pixels'\n",
    "    },\n",
    "    {\n",
    "        'object': bg_medn_120,\n",
    "        'key': 'image',\n",
    "        'title': 'Median of 120 pixels'\n",
    "    }\n",
    "])\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': bg_mean_40,\n",
    "        'key': 'image',\n",
    "        'title': 'Mean of 40 pixels'\n",
    "    },\n",
    "    {\n",
    "        'object': bg_mean_80,\n",
    "        'key': 'image',\n",
    "        'title': 'Mean of 80 pixels'\n",
    "    },\n",
    "    {\n",
    "        'object': bg_mean_120,\n",
    "        'key': 'image',\n",
    "        'title': 'Mean of 120 pixels'\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing various tests, it has been noticed that the output doesn't result in a significant change of quality by increasing the value over 80 frames, on the other hand, by decreasing the value under 80, the resulting image looks unclear using both functions.\n",
    "\n",
    "It has also been observed that the `np.median` function achieves more stable solutions.\n",
    "\n",
    "The chosen values are `80` frames using the `np.median` interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_background = bg_medn_80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Background update\n",
    "The two main approaches to obtain a dynamic background are `blind` and `selective`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section shows an example of a blind background computed using an adaption rate of `0.3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blind_backgrounds = video.process_backgrounds(BackgroundMethod.BLIND, initial_background, 0.3)\n",
    "key_frame_backgrounds = [blind_backgrounds[key_frame_index] for key_frame_index in key_frame_indexes]\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': key_frame_backgrounds[0],\n",
    "        'key': 'image',\n",
    "        'title': ''\n",
    "    },\n",
    "    {\n",
    "        'object': key_frame_backgrounds[1],\n",
    "        'key': 'image',\n",
    "        'title': ''\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the selective approach, a threshold and a distance function have to be used to compute the subtraction between the frame and the background.\n",
    "\n",
    "As a further improvement, some binary morphology operators are applied to obtain a more meaningful background mask:\n",
    "- 3x3 Opening: Denoising of the subtraction\n",
    "- 50x50 Closing: Filling of any potential holes\n",
    "- 15x15 Dilation: Achievement of a mask big enough to contain the detected objects and their position after any movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selective_backgrounds = video.process_backgrounds(\n",
    "    BackgroundMethod.SELECTIVE, initial_background, \n",
    "    0.3, \n",
    "    30, \n",
    "    distance_euclidean, \n",
    "    MorphOpsSet(\n",
    "        MorphOp(cv2.MORPH_OPEN, (3,3)), \n",
    "        MorphOp(cv2.MORPH_CLOSE, (50,50), cv2.MORPH_ELLIPSE), \n",
    "        MorphOp(cv2.MORPH_DILATE, (15,15), cv2.MORPH_ELLIPSE)\n",
    "    )\n",
    ")\n",
    "key_frame_backgrounds = [selective_backgrounds[key_frame_index] for key_frame_index in key_frame_indexes]\n",
    "\n",
    "for key_frame_background in key_frame_backgrounds:\n",
    "    key_frame_background.display_row([\n",
    "        {\n",
    "            'key': 'subtraction',\n",
    "            'title': 'Subtraction'\n",
    "        },\n",
    "        {\n",
    "            'key': 'mask_raw',\n",
    "            'title': 'Mask'\n",
    "        },\n",
    "        {\n",
    "            'key': 'mask_refined',\n",
    "            'title': 'Mask after binary morphology'\n",
    "        },\n",
    "        {\n",
    "            'key': 'image',\n",
    "            'title': 'Final Result'\n",
    "        },\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A selective background clearly obtains better results, it is therefore the chosen method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Change Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Background Subtraction\n",
    "\n",
    "To perform the actual change detection the first step is to subtract the current frame with respect to the corresponding background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 15\n",
    "distance_function = distance_euclidean\n",
    "\n",
    "\n",
    "for key_frame_index, key_frame in zip(key_frame_indexes, key_frames):\n",
    "    key_frame.apply_change_detection(selective_backgrounds[key_frame_index], threshold, distance_function)\n",
    "    key_frame.display_row([\n",
    "        {\n",
    "            'key': 'subtraction',\n",
    "            'title': 'Subtraction'\n",
    "        },\n",
    "        {\n",
    "            'key': 'mask_raw',\n",
    "            'title': 'Thresholded Mask'\n",
    "        }\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Binary Morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the selective background update, the resulting mask doesn't show a big amount of noise, a series of binary morphology operators is applied to improve furthermore the result:\n",
    "- 3x3 Rectangular Opening: Removal of the majority of the noise\n",
    "- 50x50 Closing: Execution of holes filling\n",
    "- 10x10 Opening: Deletion of small blobs due to the remaining noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1 = MorphOpsSet(\n",
    "    MorphOp(cv2.MORPH_OPEN, (3,3))\n",
    ")\n",
    "\n",
    "ms2 = MorphOpsSet(\n",
    "    MorphOp(cv2.MORPH_OPEN, (3,3)),\n",
    "    MorphOp(cv2.MORPH_CLOSE, (50, 50), cv2.MORPH_ELLIPSE),\n",
    ")\n",
    "\n",
    "ms3 = MorphOpsSet(\n",
    "    MorphOp(cv2.MORPH_OPEN, (3,3)),\n",
    "    MorphOp(cv2.MORPH_CLOSE, (50, 50), cv2.MORPH_ELLIPSE),\n",
    "    MorphOp(cv2.MORPH_OPEN, (10,10), cv2.MORPH_ELLIPSE),\n",
    ")\n",
    "\n",
    "for key_frame in key_frames:\n",
    "    frame_op1, frame_op2 = key_frame.copy(), key_frame.copy()\n",
    "    frame_op1.apply_morphology_operators(ms1)\n",
    "    frame_op2.apply_morphology_operators(ms2)\n",
    "    key_frame.apply_morphology_operators(ms3)\n",
    "    subplot_images([\n",
    "        {\n",
    "            'object': frame_op1,\n",
    "            'key': 'mask_refined',\n",
    "            'title': 'Operation'\n",
    "        },\n",
    "        {\n",
    "            'object': frame_op2,\n",
    "            'key': 'mask_refined',\n",
    "            'title': 'Operation'\n",
    "        },\n",
    "        {\n",
    "            'object': key_frame,\n",
    "            'key': 'mask_refined',\n",
    "            'title': 'Operation'\n",
    "        }\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Blob Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Blob Labeling\n",
    "\n",
    "The labeling of the obtained image is performed using the `cv2.connectedComponents` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_frame in key_frames:\n",
    "    key_frame.apply_blob_labeling()\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': key_frames[0],\n",
    "        'key': 'blobs_labeled',\n",
    "        'title': \"\",\n",
    "    },\n",
    "    {\n",
    "        'object': key_frames[1],\n",
    "        'key': 'blobs_labeled',\n",
    "        'title': \"\",\n",
    "    },\n",
    "    {\n",
    "        'object': key_frames[2],\n",
    "        'key': 'blobs_labeled',\n",
    "        'title': \"\",\n",
    "    },\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Selected Features\n",
    "\n",
    "The considered features are:\n",
    "- Area\n",
    "- Perimeter\n",
    "- Barycentre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Label Remapping\n",
    "It is necessary to keep continuity between the blob labeling in subsequential frames, therefore, a similarity function is computed to seek for correspondances between the current and previous blobs.\n",
    "\n",
    "The label are then remapped to match the ones in the previous frame.\n",
    "The blobs are displayed with their label printed on the barycentre.\n",
    "\n",
    "Given the area of the two objects $A_1, A_2$, their barycentres $(x_1, y_1), (x_2, y_2)$, the total area of the frame $A_F$ and its diagonal $d_F$ the similarity function is defined as:\n",
    "\n",
    "\n",
    "$$\\frac{\\frac{A_1-A_2}{A_F}+\\frac{\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}}{d_F}}{2}$$\n",
    "\n",
    "A threshold parameter is then applied to define a lower bound for a minimum similarity below which two blobs are always considered different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intrusiondetection.utility import default_parameters\n",
    "\n",
    "for key_frame, key_frame_index in zip(key_frames, key_frame_indexes):\n",
    "    key_frame_prev = video.frames[key_frame_index - 1]\n",
    "    key_frame_prev.intrusion_detection(default_parameters(), selective_backgrounds[key_frame_index - 1], [])\n",
    "    key_frame.apply_blob_remapping(key_frame_prev.blobs, 80)\n",
    "\n",
    "    subplot_images([\n",
    "        {\n",
    "            'object': key_frame_prev,\n",
    "            'key': 'blobs_remapped',\n",
    "            'title': 'Previous Frame Labeling'\n",
    "        },\n",
    "        {\n",
    "            'object': key_frame,\n",
    "            'key': 'blobs_remapped',\n",
    "            'title': 'Current Frame Labeling'\n",
    "        }\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Classification \n",
    "To classify the blobs as `person` or `object`, a `classification_score` is computed. The value is based on their area by normalizing it with respect to the total number of pixels in the frame.\n",
    "\n",
    "After some analysis of the video, the chosen threshold for the classification is `2.6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_frame in key_frames:\n",
    "    key_frame.apply_classification(2.6)\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': key_frames[0],\n",
    "        'key': 'blobs_classified',\n",
    "        'title': ''\n",
    "    },\n",
    "    {\n",
    "        'object': key_frames[1],\n",
    "        'key': 'blobs_classified',\n",
    "        'title': ''\n",
    "    },\n",
    "    {\n",
    "        'object': key_frames[2],\n",
    "        'key': 'blobs_classified',\n",
    "        'title': ''\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. True/False Object Recognition\n",
    "\n",
    "To detect whether an object is effectively present or not, the selected approach is the evaluation of the edge strength, such that, when the contours of a found blob presents sharp edges in the original image, it is considered to be present, otherwise it is labeled as fake.\n",
    "\n",
    "The computation is performed by using a Sobel operator returning a smoothed edge score, not taking into account the large amount of noise present in the original frame. \n",
    "\n",
    "Given the fact that in the majority of the situations, the presence/absence of the object is continuous between contiguous frames, it has been chosen to use a value obtained by computing a weighted sum between the current edge score and the score of the correspondent object in the precedent frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_frame in key_frames:\n",
    "    key_frame.apply_object_recognition(92, 0.1)\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': key_frames[0],\n",
    "        'key': 'blobs_detected',\n",
    "        'title': ''\n",
    "    },\n",
    "    {\n",
    "        'object': key_frames[1],\n",
    "        'key': 'blobs_detected',\n",
    "        'title': ''\n",
    "    },\n",
    "    {\n",
    "        'object': key_frames[2],\n",
    "        'key': 'blobs_detected',\n",
    "        'title': ''\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Output Generation\n",
    "\n",
    "### 6.1. Text Output\n",
    "\n",
    "A CSV file is then generated, for each frame the following informations are stored:\n",
    "- Frame Index\n",
    "- Number of Detected Objects\n",
    "- A row for each detected object containing:\n",
    "   - Object Identifier\n",
    "   - Area\n",
    "   - Perimeter\n",
    "   - Barycentre (x coordinate)\n",
    "   - Barycentre (y coordinate)\n",
    "   - Classification Score\n",
    "   - Edge Score\n",
    "   - Object Presence [True / False] \n",
    "   - Classification\n",
    "\n",
    "### 6.2. Video Output\n",
    "\n",
    "The graphical output shows the contours of the found objects, the color of the contour depends on the object classification:\n",
    "- Person: Blue\n",
    "- True Object: Green\n",
    "- Fake Object: Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_frame in key_frames:\n",
    "    key_frame.generate_graphical_output()\n",
    "\n",
    "subplot_images([\n",
    "    {\n",
    "        'object': key_frames[0],\n",
    "        'key': 'image_output',\n",
    "        'title': ''\n",
    "    },\n",
    "    {\n",
    "        'object': key_frames[1],\n",
    "        'key': 'image_output',\n",
    "        'title': ''\n",
    "    },\n",
    "    {\n",
    "        'object': key_frames[2],\n",
    "        'key': 'image_output',\n",
    "        'title': ''\n",
    "    }\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
